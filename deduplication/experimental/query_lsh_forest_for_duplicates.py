from datasketch import MinHash, MinHashLSH, MinHashLSHForest
from tqdm.autonotebook import tqdm
from multiprocessing import Pool
from glob import glob
import argparse
import pickle
import json
import csv
import sys
import numpy as np
import time


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input",
        help=f"Input directory with precomputed minhash signatures",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--query",
        help=f"Directory with precomputed minhash signatures to compare against",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--output", help=f"Output csv file path", type=str, required=True
    )
    # parser.add_argument("--nodeid", help=f"Idx of the current node", type=str, required=False, default="00")
    # parser.add_argument("--redis_port", help=f"Port for local Redis server. Default is 6379.", type=int, required=False, default=6379)

    return parser.parse_args()


"""
Takes the list of bytes-like generated by the LSH Forest 
that corresponds to some given key and recovers the hashvalues
which can be converted back into a MinHash to compute Jaccard Similarity

Given a number of prefix trees, L, when we insert a (key, MinHash) pair 
the LSH Forest creates L byteslike items each corresponding to a range of 
hashvalues from the original MinHash object for a given key. Each range is of
size num_perm / L. Therefore here we convert these items from byteslikes back into
arrays of unsigned integers and then concatenate them so that they are in a representation
that we can build a MinHash object with. Namely, we return an array of unsigned integers
of length num_perm that represent hashvalues from each of num_perm hash functions
chosen during the MinHash creation.
"""


def byteslist_to_hashvalues(byteslist):
    hashvalues = np.array([], dtype=np.uint64)
    for item in byteslist:
        # unswap the bytes, as their representation is flipped during storage
        hv_segment = np.frombuffer(item, dtype=np.uint64).byteswap()
        hashvalues = np.append(hashvalues, hv_segment)

    return hashvalues


def query(params: tuple) -> list:
    # find top-k nearest neighbors by estimated jaccard similarity
    key, m_query = params
    result = lsh.query(m_query, k=5)
    duplicates = []

    # screen for results from the top-k that exceed our similarity threshold
    if len(result):
        for dup_key in result:
            hash_values_bytes = lsh.keys[dup_key]
            hash_values = byteslist_to_hashvalues(hash_values_bytes)
            dup_mh = MinHash(hashvalues=hash_values)
            if dup_mh.jaccard(m_query) >= sim_threshold:
                duplicates.append((key, dup_key))

    return duplicates


def load_minhashes_to_forest(indir: str):
    for minhashfile in glob(f"{indir}/*.pkl"):
        with open(minhashfile, "rb") as fin:
            minhash_list = pickle.load(fin)
            for item in minhash_list:
                key, minhash = item
                lsh.add(key, minhash)


if __name__ == "__main__":
    start_program = time.perf_counter()

    args = parse_args()

    sim_threshold = 0.8
    n_hash_funcs = 128
    lsh = MinHashLSHForest(num_perm=n_hash_funcs, l=8)

    outfile = args.output
    indir = args.input
    querydir = args.query

    # load minhashes into forest to query against
    s = time.perf_counter()
    load_minhashes_to_forest(indir)
    secs_read_minhashes = time.perf_counter() - s

    # this function must be called to index the forest, otherwise search operations
    # will return no results
    s = time.perf_counter()
    lsh.index()
    secs_index_forest = time.perf_counter() - s

    is_empty = lsh.is_empty()
    if is_empty:
        raise Exception(
            f"LSH Forest is empty (likely could not load minhash signatures from path: {indir})"
        )

    # query minhashes from args.query against forest
    s = time.perf_counter()
    avg_time_query_file = 0
    n = 0
    with open(outfile, "w") as fout:
        writer = csv.writer(fout)
        for minhashfile in glob(f"{querydir}/*.pkl"):
            s_file = time.perf_counter()
            with open(minhashfile, "rb") as fin:
                minhash_list = pickle.load(fin)
                fname = minhashfile.split("/")[-1]
                with Pool(32) as p, tqdm(total=len(minhash_list), desc=fname) as pbar:
                    for result in p.imap_unordered(query, minhash_list):
                        if result:
                            writer.writerows(result)
                        pbar.update()

            elapsed_file = time.perf_counter() - s_file
            avg_time_query_file += elapsed_file
            n += 1

    avg_time_query_file /= n
    secs_query_all = time.perf_counter() - s
    elapsed_program = time.perf_counter() - start_program

    print("\n")
    print(f"Time to load minhashes: {secs_read_minhashes} seconds")
    print(f"Time to index forest: {secs_index_forest} seconds")
    print(f"Average time to query pickle file: {avg_time_query_file} seconds")
    print(f"Time to query all {n} pickle files: {secs_query_all} seconds")
    print(f"Total program runtime: {elapsed_program} seconds")
    print("\n")
